{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Manually testing data pipelines",
   "id": "c1aef4912474dfae"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Validating a data pipeline at \"checkpoints\"\n",
    "In this exercise, you'll be working with a data pipeline that extracts tax data from a CSV file, creates a new column, filters out rows based on average taxable income, and persists the data to a parquet file.\n",
    "\n",
    "pandas has been loaded as pd, and the extract(), transform(), and load() functions have already been defined. You'll use these functions to validate the data pipeline at various checkpoints throughout its execution."
   ],
   "id": "96021abf9cc1c868"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T15:00:04.401588Z",
     "start_time": "2025-11-03T15:00:03.694201Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "def extract(file_path):\n",
    "    \"\"\"Extract data from a CSV file.\"\"\"\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "def transform(df):\n",
    "    clean_df = df.copy()\n",
    "\n",
    "    clean_df = clean_df.dropna()\n",
    "\n",
    "    if 'taxable_income' in clean_df.columns:\n",
    "        avg_income = clean_df['taxable_income'].mean()\n",
    "        clean_df = clean_df[clean_df['taxable_income'] >= avg_income]\n",
    "\n",
    "    return clean_df\n",
    "def load(df, output_path):\n",
    "    \"\"\"Load the DataFrame to a parquet file.\"\"\"\n",
    "    df.to_parquet(output_path, index=False)\n",
    "    print(f\"Data successfully loaded to {output_path}\")"
   ],
   "id": "d4d8d4bd8ff76548",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T15:01:05.499217Z",
     "start_time": "2025-11-03T15:01:05.474186Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Extract and transform tax_data\n",
    "raw_tax_data = extract(\"../data/raw_tax_data.csv\")\n",
    "clean_tax_data = transform(raw_tax_data)\n",
    "\n",
    "# Check the shape of the raw_tax_data DataFrame, compare to the clean_tax_data DataFrame\n",
    "print(f\"Shape of raw_tax_data: {raw_tax_data.shape}\")\n",
    "print(f\"Shape of clean_tax_data: {clean_tax_data.shape}\")"
   ],
   "id": "4564e90c729ea521",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of raw_tax_data: (82, 6)\n",
      "Shape of clean_tax_data: (82, 6)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T15:11:04.314466Z",
     "start_time": "2025-11-03T15:11:04.202096Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the clean_tax_data to parquet file\n",
    "load(clean_tax_data, \"../data/clean_tax_data.parquet\")\n",
    "\n",
    "# Read in the loaded data, observe the head of each\n",
    "to_validate = pd.read_parquet(\"../data/clean_tax_data.parquet\")\n",
    "print(clean_tax_data.head(3))\n",
    "print(to_validate.head(3))"
   ],
   "id": "d7e7e9e3d8deee4b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully loaded to ../data/clean_tax_data.parquet\n",
      "       industry_name  number_of_firms  total_taxable_income  total_taxes_paid  \\\n",
      "0  Aerospace/Defense               77               30920.0          5106.376   \n",
      "1            Apparel               39                5423.0          1112.113   \n",
      "2       Auto & Truck               31               33360.0          3529.000   \n",
      "\n",
      "   total_cash_taxes_paid  average_taxable_income  \n",
      "0               7441.776                 401.561  \n",
      "1               1479.292                 139.043  \n",
      "2               2446.896                1076.071  \n",
      "       industry_name  number_of_firms  total_taxable_income  total_taxes_paid  \\\n",
      "0  Aerospace/Defense               77               30920.0          5106.376   \n",
      "1            Apparel               39                5423.0          1112.113   \n",
      "2       Auto & Truck               31               33360.0          3529.000   \n",
      "\n",
      "   total_cash_taxes_paid  average_taxable_income  \n",
      "0               7441.776                 401.561  \n",
      "1               1479.292                 139.043  \n",
      "2               2446.896                1076.071  \n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T15:11:07.093733Z",
     "start_time": "2025-11-03T15:11:07.079614Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Read in the loaded data, observe the head of each\n",
    "to_validate = pd.read_parquet(\"../data/clean_tax_data.parquet\")\n",
    "print(clean_tax_data.head(3))\n",
    "print(to_validate.head(3))\n"
   ],
   "id": "789e54fa8e334d9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       industry_name  number_of_firms  total_taxable_income  total_taxes_paid  \\\n",
      "0  Aerospace/Defense               77               30920.0          5106.376   \n",
      "1            Apparel               39                5423.0          1112.113   \n",
      "2       Auto & Truck               31               33360.0          3529.000   \n",
      "\n",
      "   total_cash_taxes_paid  average_taxable_income  \n",
      "0               7441.776                 401.561  \n",
      "1               1479.292                 139.043  \n",
      "2               2446.896                1076.071  \n",
      "       industry_name  number_of_firms  total_taxable_income  total_taxes_paid  \\\n",
      "0  Aerospace/Defense               77               30920.0          5106.376   \n",
      "1            Apparel               39                5423.0          1112.113   \n",
      "2       Auto & Truck               31               33360.0          3529.000   \n",
      "\n",
      "   total_cash_taxes_paid  average_taxable_income  \n",
      "0               7441.776                 401.561  \n",
      "1               1479.292                 139.043  \n",
      "2               2446.896                1076.071  \n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Check that the DataFrames are equal\n",
    "print(to_validate.equals(clean_tax_data))\n"
   ],
   "id": "358cd9b095d13307"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Testing a data pipeline end-to-end\n",
    "In this exercise, you'll be working with the same data pipeline as before, which extracts, transforms, and loads tax data. You'll practice testing this pipeline end-to-end to ensure the solution can be run multiple times, without duplicating the transformed data in the parquet file.\n",
    "\n",
    "pandas has been loaded as pd, and the extract(), transform(), and load() functions have already been defined."
   ],
   "id": "fcde65ef5ffbf3ca"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Trigger the data pipeline to run three times\n",
    "for attempt in range(0, 3):\n",
    "\tprint(f\"Attempt: {attempt}\")\n",
    "\traw_tax_data = extract(\"raw_tax_data.csv\")\n",
    "\tclean_tax_data = transform(raw_tax_data)\n",
    "\tload(clean_tax_data, \"clean_tax_data.parquet\")\n",
    "\n",
    "\t# Print the shape of the cleaned_tax_data DataFrame\n",
    "\tprint(f\"Shape of clean_tax_data: {clean_tax_data.shape}\")\n",
    "\n",
    "# Read in the loaded data, check the shape\n",
    "to_validate = pd.read_parquet(\"../data/clean_tax_data.parquet\")\n",
    "print(f\"Final shape of cleaned data: {to_validate.shape}\")\n"
   ],
   "id": "fb374746be084925"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Unit-testing a data pipeline",
   "id": "ef4b427b59d15ec5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Validating a data pipeline with assert\n",
    "To build unit tests for data pipelines, it's important to get familiar with the assert keyword, and the isinstance() function. In this exercise, you'll practice using these two tools to validate components of a data pipeline.\n",
    "\n",
    "The functions extract() and transform() have been made available for you, along with pandas, which has been imported as pd. Both extract() and transform() return a DataFrame. Good luck!"
   ],
   "id": "53a7d88b2366fe24"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T15:27:31.918030Z",
     "start_time": "2025-11-03T15:27:31.910894Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract(file_path):\n",
    "    \"\"\"Extract data from a CSV file.\"\"\"\n",
    "    return pd.read_csv(file_path)"
   ],
   "id": "b81c7d62f7abd8ad",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T15:30:06.756587Z",
     "start_time": "2025-11-03T15:30:06.741453Z"
    }
   },
   "cell_type": "code",
   "source": [
    "raw_tax_data = extract(\"../data/raw_tax_data.csv\")\n",
    "clean_tax_data = transform(raw_tax_data)\n",
    "\n",
    "# Validate the number of columns in the DataFrame\n",
    "assert len(clean_tax_data.columns) == 6"
   ],
   "id": "8c92825c7505bb47",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T15:31:51.485741Z",
     "start_time": "2025-11-03T15:31:51.478062Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Determine if the clean_tax_data DataFrames take type pd.DataFrame\n",
    "assert isinstance(clean_tax_data, pd.DataFrame)\n"
   ],
   "id": "b9a77b940abb6f38",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Assert that clean_tax_data is an instance of a pd.DataFrame\n",
    "assert isinstance(clean_tax_data, pd.DataFrame)\n"
   ],
   "id": "2ea28545900b34c6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Assert that clean_tax_data takes is an instance of a string\n",
    "try:\n",
    "\tassert isinstance(clean_tax_data, str)\n",
    "except Exception as e:\n",
    "\tprint(e)\n"
   ],
   "id": "6d7bad1a4442780d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Writing unit tests with pytest\n",
    "In this exercise, you'll practice writing a unit test to validate a data pipeline. You'll use assert and other tools to build the tests, and determine if the data pipeline performs as it should.\n",
    "\n",
    "The functions extract() and transform() have been made available for you, along with pandas, which has been imported as pd. You'll be testing the transform() function, which is shown below.\n",
    "\n",
    "def transform(raw_data):\n",
    "    raw_data[\"average_taxable_income\"] = raw_data[\"total_taxable_income\"] / raw_data[\"number_of_firms\"]\n",
    "    clean_data = raw_data.loc[raw_data[\"average_taxable_income\"] > 100, :]\n",
    "    clean_data.set_index(\"industry_name\", inplace=True)\n",
    "    return clean_data"
   ],
   "id": "c109b1f423a6f41d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T15:35:54.343733Z",
     "start_time": "2025-11-03T15:35:54.263865Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pytest\n",
    "\n",
    "def test_transformed_data():\n",
    "    raw_tax_data = extract(\"../data/raw_tax_data.csv\")\n",
    "    clean_tax_data = transform(raw_tax_data)\n",
    "\n",
    "    # Assert that the transform function returns a pd.DataFrame\n",
    "    assert isinstance(clean_tax_data, pd.DataFrame)\n",
    "\n",
    "    # Assert that the clean_tax_data DataFrame has more columns than the raw_tax_data DataFrame\n",
    "    assert len(clean_tax_data.columns) > len(raw_tax_data.columns)\n"
   ],
   "id": "816d8e7cede349ad",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Creating fixtures with pytest\n",
    "When building unit tests, you'll sometimes have to do a bit of setup before testing can begin. Doing this setup within a unit test can make the tests more difficult to read, and may have to be repeated several times. Luckily, pytest offers a way to solve these problems, with fixtures.\n",
    "\n",
    "For this exercise, pandas has been imported as pd, and the extract() function shown below is available for use!\n",
    "\n",
    "def extract(file_path):\n",
    "    return pd.read_csv(file_path)"
   ],
   "id": "5382c3be955a23bf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T15:36:35.242449Z",
     "start_time": "2025-11-03T15:36:35.237049Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import pytest\n",
    "import pytest\n",
    "\n",
    "# Create a pytest fixture\n",
    "@pytest.fixture()\n",
    "def raw_tax_data():\n",
    "    # Extract raw data from CSV file\n",
    "\traw_data = extract(\"../data/raw_tax_data.csv\")\n",
    "\n",
    "    # Return the raw DataFrame\n",
    "\treturn raw_data\n"
   ],
   "id": "836d76b1e58266b9",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Unit testing a data pipeline with fixtures\n",
    "You've learned in the last video that unit testing can help to instill more trust in your data pipeline, and can even help to catch bugs throughout development. In this exercise, you'll practice writing both fixtures and unit tests, using the pytest library and assert.\n",
    "\n",
    "The transform function that you'll be building unit tests around is shown below for reference. pandas has been imported as pd, and the pytest() library is loaded and ready for use.\n",
    "\n",
    "def transform(raw_data):\n",
    "    raw_data[\"tax_rate\"] = raw_data[\"total_taxes_paid\"] / raw_data[\"total_taxable_income\"]\n",
    "    raw_data.set_index(\"industry_name\", inplace=True)\n",
    "    return raw_data"
   ],
   "id": "63f7b5e5301f9232"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T15:39:57.897704Z",
     "start_time": "2025-11-03T15:39:57.890149Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define a pytest fixture\n",
    "@pytest.fixture()\n",
    "def clean_tax_data():\n",
    "    raw_data = pd.read_csv(\"raw_tax_data.csv\")\n",
    "\n",
    "    # Transform the raw_data, store in clean_data DataFrame, and return the variable\n",
    "    clean_data = transform(raw_data)\n",
    "    return clean_data"
   ],
   "id": "826f865675cf5b6d",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Pass the fixture to the function\n",
    "def test_tax_rate(clean_tax_data):\n",
    "    # Assert values are within the expected range\n",
    "    assert clean_tax_data[\"tax_rate\"].max() <= 1 and clean_tax_data[\"tax_rate\"].min() >= 0\n"
   ],
   "id": "d0d85c63cbeb2729"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Running a data pipeline in production",
   "id": "3f4562bca0b1ac58"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Data pipeline architecture patterns\n",
    "When building data pipelines, it's best to separate the files where functions are being defined from where they are being run.\n",
    "\n",
    "In this exercise, you'll practice importing components of a pipeline into memory before using these functions to run the pipeline end-to-end. The project takes the following format, where pipeline_utils stores the extract(), transform(), and load() functions that will be used run the pipeline.\n"
   ],
   "id": "cfac0442f5623798"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Import the extract, transform, and load functions\n",
    "\n",
    "# Run the pipeline end to end by extracting, transforming and loading the data\n",
    "raw_tax_data = extract(\"../data/raw_tax_data.csv\")\n",
    "clean_tax_data = transform(raw_tax_data)\n",
    "load(clean_tax_data, \"../data/clean_tax_data.parquet\")\n"
   ],
   "id": "d407698ae4fb5d52"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Running a data pipeline end-to-end\n",
    "It's important to monitor the performance of a pipeline when running in production. Earlier in the course, you explored tools such as exception handling and logging. In this last exercise, we'll practice running a pipeline end-to-end, while monitoring for exceptions and logging performance."
   ],
   "id": "4154c0b58e0b96cf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T15:51:28.377281Z",
     "start_time": "2025-11-03T15:51:28.368310Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(levelname)s: %(message)s', level=logging.DEBUG)\n",
    "\n",
    "try:\n",
    "  \t# Extract, transform, and load the tax data\n",
    "    raw_tax_data = extract(\"../data/raw_tax_data.csv\")\n",
    "    clean_tax_data = transform(raw_tax_data)\n",
    "    load(clean_tax_data, \"../data/clean_tax_data.parquet\")\n",
    "\n",
    "\tlogging.info(\"Successfully extracted, transformed and loaded data.\")  # Log a success message.\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"Pipeline failed with error: {e}\")  # Log failure message\n"
   ],
   "id": "657be4c3479aa9f4",
   "outputs": [],
   "execution_count": 24
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
